# -*- coding: utf-8 -*-
"""Dataset_Appliances-Energy-Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oeftaqW485f6rk4GAKg1zUzhbapSCNH_

# Importação dos Dados
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

"""## 26. Carregamento e inspeção inicial
Carregue o dataset no Pandas.
Liste tipos de dados e estatísticas descritivas (.info() e describe()).
"""

# Carregamento do dataset
df = pd.read_csv('/content/energydata_complete.csv')

# Listagem dos tipos de dados e estatísticas descritivas
df.info()
display(df.describe())

"""## 27. Distribuição do consumo
Crie histogramas e séries temporais para a variável Appliances.
- Pergunta: o consumo tende a se concentrar em valores baixos ou altos?
"""

# Create a histogram of the 'Appliances' variable
plt.figure(figsize=(10, 6))
sns.histplot(df['Appliances'], kde=True)
plt.title('Distribution of Appliance Consumption')
plt.xlabel('Appliances (Wh)')
plt.ylabel('Frequency')
plt.show()

# Convert the 'date' column to datetime objects
df['date'] = pd.to_datetime(df['date'])

# Create a time series plot of the 'Appliances' variable
plt.figure(figsize=(15, 7))
sns.lineplot(x='date', y='Appliances', data=df)
plt.title('Time Series of Appliance Consumption')
plt.xlabel('Date')
plt.ylabel('Appliances (Wh)')
plt.show()

# Baseado no histograma, o consumo tende a ser concentrado em valores baixos,
# com um grande caminho se estendendo para valores de consumo mais altos.
#O gráfico de série temporal mostra variações no consumo ao longo do tempo.

"""## 28. Correlações com variáveis ambientais
- Calcule correlações entre Appliances e variáveis como temperatura e umidade.
Pergunta: quais fatores têm mais relação com o consumo?
"""

# Calculate correlations between 'Appliances' and other variables
correlations = df.corr()['Appliances'].sort_values(ascending=False)

# Display the correlations
display(correlations)

# Com base nos valores de correlação, as variáveis com a correlação positiva mais forte com 'Appliances' são 'lights', 'T2' e 'T6'.
# Por outro lado, 'RH_out', 'RH_8' e 'RH_6' mostram a correlação negativa mais forte.
# Isso sugere que o uso de iluminação e as temperaturas em certas áreas ('T2', 'T6') estão positivamente relacionados ao consumo de eletrodomésticos,
# enquanto a umidade externa e a umidade em certos cômodos ('RH_8', 'RH_6') estão negativamente relacionadas.

"""## 29. Normalização dos dados
- Aplique Min-Max Scaling as variáveis numéricas.
- Reutilize esses dados em modelos posteriores.
"""

# Select numerical columns, excluding 'Appliances' as it's the target variable
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
numerical_cols.remove('Appliances')
if 'lights' in numerical_cols:
    numerical_cols.remove('lights') # Removing 'lights' as it might be considered a feature, not a target for now

# Apply Min-Max Scaling
scaler = MinMaxScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Display the first few rows of the scaled data
display(df.head())

"""## 30. PCA
Aplique PCA e reduza para 2 componentes principais.
- Plote os dados resultantes.
Pergunta: aparecem padrões ou agrupamentos naturais?
"""

# Select only the numerical columns that were scaled (excluding 'date', 'Appliances', 'lights')
scaled_numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
scaled_numerical_cols.remove('Appliances')
if 'lights' in scaled_numerical_cols:
    scaled_numerical_cols.remove('lights')
if 'date' in scaled_numerical_cols:
    scaled_numerical_cols.remove('date')


df_scaled_numerical = df[scaled_numerical_cols]

# Apply PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(df_scaled_numerical)

# Create a DataFrame with the principal components
pca_df = pd.DataFrame(data = principal_components, columns = ['principal component 1', 'principal component 2'])

# Plot the PCA results
plt.figure(figsize=(10, 6))
sns.scatterplot(x='principal component 1', y='principal component 2', data=pca_df, alpha=0.5)
plt.title('PCA of Appliance Consumption Data')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

# Analisando o gráfico de PCA,
# não parece haver agrupamentos naturais distintos ou padrões claros visíveis nos dados projetados para os dois
# primeiros componentes principais.
# Os pontos estão bastante dispersos.

"""# 31. Regressão linear Múltipla
- Modele Appliances em função das variáveis ambientais

- Avalie R² e erro médio
"""

# Definir features (X) e target (y)
# Excluir as colunas 'date', 'Appliances' e 'lights' conforme a descrição da tarefa para modelar 'Appliances' como uma função das variáveis ambientais
X = df.drop(['date', 'Appliances', 'lights'], axis=1)
y = df['Appliances']

display(X.head())
display(y.head())

"""# 32. Random Forest Regressor
- Treine um modelo de Random Forest para prever Appliances.

- Compare o RMSE com a regressão linear.
"""

# Dividir os dados em conjuntos de treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""# 33. K-Means clustering
- Aplique K-Means com 3 a 5 clusters.

- Interprete os perfis de consumo.
"""

# Selecionar colunas numéricas para agrupamento, excluindo 'Appliances' e 'lights'
numerical_cols_clustering = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
numerical_cols_clustering.remove('Appliances')
if 'lights' in numerical_cols_clustering:
    numerical_cols_clustering.remove('lights')

X_clustering = df[numerical_cols_clustering]

display(X_clustering.head())

"""# 34. Classificação Binária
- Crie uma variável: alto vs baixo consumo (Aplliances maior/menor que a mediana).
- Treine Logistic Regression e Random Forest Classifier.

"""

# Criar uma variável alvo binária: consumo alto vs baixo (Appliances maior/menor que a mediana)
median_appliances = df['Appliances'].median()
df['high_consumption'] = (df['Appliances'] > median_appliances).astype(int)

display(df[['Appliances', 'high_consumption']].head())
display(df['high_consumption'].value_counts())

"""# 35. Avaliação de classificação
- Gere matriz de confusão e métricas (accuracy, precision, ecall, F1-score).

- Pergunta: o modelo erra mais para alto ou para baixo consumo?
"""

# Treinar o modelo de Regressão Logística
log_reg_model = LogisticRegression(random_state=42)
log_reg_model.fit(X_train_clf, y_train_clf)

# Treinar o modelo Random Forest Classifier
rf_clf_model = RandomForestClassifier(random_state=42)
rf_clf_model.fit(X_train_clf, y_train_clf)

# Fazer previsões nos dados de teste
y_pred_log_reg = log_reg_model.predict(X_test_clf)
y_pred_rf_clf = rf_clf_model.predict(X_test_clf)

# Avaliar o modelo de Regressão Logística
print("--- Avaliação do Modelo de Regressão Logística ---")
print("Matriz de Confusão:")
display(confusion_matrix(y_test_clf, y_pred_log_reg))
accuracy_log_reg = accuracy_score(y_test_clf, y_pred_log_reg)
precision_log_reg = precision_score(y_test_clf, y_pred_log_reg)
recall_log_reg = recall_score(y_test_clf, y_pred_log_reg)
f1_log_reg = f1_score(y_test_clf, y_pred_log_reg)

print(f"Acurácia: {accuracy_log_reg:.4f}")
print(f"Precisão: {precision_log_reg:.4f}")
print(f"Recall: {recall_log_reg:.4f}")
print(f"F1-score: {f1_log_reg:.4f}")

print("\n--- Avaliação do Modelo Random Forest Classifier ---")
print("Matriz de Confusão:")
display(confusion_matrix(y_test_clf, y_pred_rf_clf))
accuracy_rf_clf = accuracy_score(y_test_clf, y_pred_rf_clf)
precision_rf_clf = precision_score(y_test_clf, y_pred_rf_clf)
recall_rf_clf = recall_score(y_test_clf, y_pred_rf_clf)
f1_rf_clf = f1_score(y_test_clf, y_pred_rf_clf)

print(f"Acurácia: {accuracy_rf_clf:.4f}")
print(f"Precisão: {precision_rf_clf:.4f}")
print(f"Recall: {recall_rf_clf:.4f}")
print(f"F1-score: {f1_rf_clf:.4f}")

# Interpretação da matriz de confusão (em português)
print("\n--- Interpretação ---")
print("Analisando as matrizes de confusão:")
print("- Para a Regressão Logística:")
print(f"  - Verdadeiros Positivos (Alto Consumo Previsto Corretamente): {confusion_matrix(y_test_clf, y_pred_log_reg)[1, 1]}")
print(f"  - Verdadeiros Negativos (Baixo Consumo Previsto Corretamente): {confusion_matrix(y_test_clf, y_pred_log_reg)[0, 0]}")
print(f"  - Falsos Positivos (Baixo Consumo Previsto como Alto): {confusion_matrix(y_test_clf, y_pred_log_reg)[0, 1]}")
print(f"  - Falsos Negativos (Alto Consumo Previsto como Baixo): {confusion_matrix(y_test_clf, y_pred_log_reg)[1, 0]}")

print("\n- Para o Random Forest Classifier:")
print(f"  - Verdadeiros Positivos (Alto Consumo Previsto Corretamente): {confusion_matrix(y_test_clf, y_pred_rf_clf)[1, 1]}")
print(f"  - Verdadeiros Negativos (Baixo Consumo Previsto Corretamente): {confusion_matrix(y_test_clf, y_pred_rf_clf)[0, 0]}")
print(f"  - Falsos Positivos (Baixo Consumo Previsto como Alto): {confusion_matrix(y_test_clf, y_pred_rf_clf)[0, 1]}")
print(f"  - Falsos Negativos (Alto Consumo Previsto como Baixo): {confusion_matrix(y_test_clf, y_pred_rf_clf)[1, 0]}")

print("\nComparando os erros (Falsos Positivos vs Falsos Negativos), podemos determinar para qual classe o modelo erra mais.")